{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1> <b>Import libraries </b> </h1>","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport torch\nimport numpy as np\nimport seaborn as sn\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torch import nn\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom glob import glob\nfrom gc import collect\nfrom pandas import read_csv\nfrom scipy.io import loadmat\nfrom torch.cuda import empty_cache\nfrom torch.optim import SGD, RMSprop\nfrom torch.optim.lr_scheduler import StepLR\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom sklearn.metrics import (\n    confusion_matrix, classification_report,\n    roc_curve, auc\n)\nfrom torch.nn import (\n    Module, ReLU, Linear, Conv2d, Softmax,\n    CrossEntropyLoss, Sequential, Sigmoid, BatchNorm2d,\n    MaxPool2d, AdaptiveAvgPool2d, MSELoss\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:09.261797Z","iopub.execute_input":"2022-05-17T08:15:09.262451Z","iopub.status.idle":"2022-05-17T08:15:11.741181Z","shell.execute_reply.started":"2022-05-17T08:15:09.262367Z","shell.execute_reply":"2022-05-17T08:15:11.740339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b>Paths </b> </h1>","metadata":{}},{"cell_type":"code","source":"IMG_PATH = \"../input/hands-and-palm-images-dataset/Hands/Hands/\" #whole images\nLABEL_PATH = \"../input/hands-and-palm-images-dataset/HandInfo.csv\" #information of images\nOUT_PATH =  \"../input/mat-files-of-hand-images/\" #low-frequency and high-frequency images\nEYTH_IMG_PATH = \"../input/eythdataset/eyth_dataset/images/\" #images for segmentation\nEYTH_MASK_PATH = \"../input/eythdataset/eyth_dataset/masks/\" #masks of images for segmentation","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:11.743054Z","iopub.execute_input":"2022-05-17T08:15:11.743579Z","iopub.status.idle":"2022-05-17T08:15:11.749794Z","shell.execute_reply.started":"2022-05-17T08:15:11.743541Z","shell.execute_reply":"2022-05-17T08:15:11.749149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GREEN = \"\\033[1;32;47m\"\nRED = \"\\033[1;31;47m\"","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:11.751826Z","iopub.execute_input":"2022-05-17T08:15:11.752698Z","iopub.status.idle":"2022-05-17T08:15:11.765306Z","shell.execute_reply.started":"2022-05-17T08:15:11.752647Z","shell.execute_reply":"2022-05-17T08:15:11.762176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b>Means and Standard deviations </b> </h1>","metadata":{}},{"cell_type":"code","source":"meanImg1, meanImg2, meanImg3 = 240.42/255, 229.6/255, 223.2/255 #mean of pixels of images\nmeanLow1, meanLow2, meanLow3 = 240.45/255, 229.64/255, 223.24/255 #mean of pixels of low-frequency images\nmeanHigh = 245.56/255 #mean of pixels of high-frequency images\nmeanSeg1, meanSeg2, meanSeg3 = 98.17, 91.66, 90.51 #mean of pixels of images for segmentation","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:11.770457Z","iopub.execute_input":"2022-05-17T08:15:11.770905Z","iopub.status.idle":"2022-05-17T08:15:11.779013Z","shell.execute_reply.started":"2022-05-17T08:15:11.770845Z","shell.execute_reply":"2022-05-17T08:15:11.776687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stdImg1, stdImg2, stdImg3 = 32.82/255, 48.86/255, 59.31/255 #std of pixels of images\nstdLow1, stdLow2, stdLow3 = 32.38/255, 48.41/255, 58.85/255 #std of pixels of low-frequency images\nstdHigh = 28.11/255 #std of pixels of high-frequency images \nstdSeg1, stdSeg2, stdSeg3 = 79.69, 77.25, 77.07 #std of pixels of images for segmentation","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:11.781836Z","iopub.execute_input":"2022-05-17T08:15:11.782252Z","iopub.status.idle":"2022-05-17T08:15:11.788454Z","shell.execute_reply.started":"2022-05-17T08:15:11.782206Z","shell.execute_reply":"2022-05-17T08:15:11.787564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b>Use GPU </b> </h1>","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\")","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:11.789616Z","iopub.execute_input":"2022-05-17T08:15:11.790078Z","iopub.status.idle":"2022-05-17T08:15:11.804942Z","shell.execute_reply.started":"2022-05-17T08:15:11.790039Z","shell.execute_reply":"2022-05-17T08:15:11.803984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Function for extract low or high-frequency image </b> </h1>","metadata":{}},{"cell_type":"code","source":"def getPartOfMat(mat,status):\n    if status == 'low':\n        return mat['O'][:,:,:3]\n    elif status == 'high':\n        return mat['O'][:,:,3]\n    elif status == \"normal\":\n        return mat['O']","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:11.806224Z","iopub.execute_input":"2022-05-17T08:15:11.806623Z","iopub.status.idle":"2022-05-17T08:15:11.816287Z","shell.execute_reply.started":"2022-05-17T08:15:11.806585Z","shell.execute_reply":"2022-05-17T08:15:11.815519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Get images </b> </h1>","metadata":{}},{"cell_type":"code","source":"images = glob(IMG_PATH + \"*.jpg\")\nimages.sort()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:11.824214Z","iopub.execute_input":"2022-05-17T08:15:11.824641Z","iopub.status.idle":"2022-05-17T08:15:12.122204Z","shell.execute_reply.started":"2022-05-17T08:15:11.824608Z","shell.execute_reply":"2022-05-17T08:15:12.121517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Get images for Segmentation </b> </h1>","metadata":{}},{"cell_type":"code","source":"segImages = glob(EYTH_IMG_PATH + '*.jpg')\nsegImages.sort()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:12.123682Z","iopub.execute_input":"2022-05-17T08:15:12.124161Z","iopub.status.idle":"2022-05-17T08:15:12.247852Z","shell.execute_reply.started":"2022-05-17T08:15:12.124123Z","shell.execute_reply":"2022-05-17T08:15:12.247201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Get mask images for Segmentation </b> </h1>","metadata":{}},{"cell_type":"code","source":"maskImages = glob(EYTH_MASK_PATH + \"*\")\nmaskImages.sort()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:12.256164Z","iopub.execute_input":"2022-05-17T08:15:12.256768Z","iopub.status.idle":"2022-05-17T08:15:12.404346Z","shell.execute_reply.started":"2022-05-17T08:15:12.256731Z","shell.execute_reply":"2022-05-17T08:15:12.403709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Extracting information and labels of images </b> </h1>","metadata":{}},{"cell_type":"code","source":"details = read_csv(LABEL_PATH)\ndetails","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:12.405385Z","iopub.execute_input":"2022-05-17T08:15:12.405739Z","iopub.status.idle":"2022-05-17T08:15:12.463451Z","shell.execute_reply.started":"2022-05-17T08:15:12.405706Z","shell.execute_reply":"2022-05-17T08:15:12.462787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Show stats information of images </b> </h1>","metadata":{}},{"cell_type":"code","source":"for col in details.columns:\n    if col == \"id\" or col == \"imageName\" or col == \"age\": continue\n    types = {}\n    for row in details[col]:\n        if row in types:\n            types[row] += 1\n            continue\n        else:\n            types[row] = 0\n                \n    plt.title(\"Information of \" + col.capitalize())\n    \n    withoutXLabels = [\"accessories\", \"nailPolish\", \"irregularities\"]\n\n    if col in withoutXLabels:\n        types[\"Without \" + col.capitalize()] = types.pop(0)\n        types[\"With \"+ col.capitalize()] = types.pop(1)\n        \n    else:\n        plt.xlabel(col.capitalize())\n\n    plt.ylabel('Number')\n        \n    plt.bar(range(len(types)), list(types.values()), tick_label=list(types.keys()), color = 'gray')\n\n    plt.ylim(ymin=types[min(types, key=types.get)] * 0.5)\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:12.464532Z","iopub.execute_input":"2022-05-17T08:15:12.464766Z","iopub.status.idle":"2022-05-17T08:15:13.663741Z","shell.execute_reply.started":"2022-05-17T08:15:12.464733Z","shell.execute_reply":"2022-05-17T08:15:13.662945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b>Get low and high-frequency images</b> </h1>","metadata":{}},{"cell_type":"code","source":"outs = glob(OUT_PATH + '*.mat')\nouts.sort()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:13.66811Z","iopub.execute_input":"2022-05-17T08:15:13.670169Z","iopub.status.idle":"2022-05-17T08:15:13.957139Z","shell.execute_reply.started":"2022-05-17T08:15:13.67013Z","shell.execute_reply":"2022-05-17T08:15:13.956257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Delete redundant Cache</b> </h1>","metadata":{}},{"cell_type":"code","source":"def deleteRedundantCache(net):\n    collect()\n    for p in net.parameters():\n        if p.grad is not None:\n            del p.grad\n    empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:13.958503Z","iopub.execute_input":"2022-05-17T08:15:13.958789Z","iopub.status.idle":"2022-05-17T08:15:13.963974Z","shell.execute_reply.started":"2022-05-17T08:15:13.958754Z","shell.execute_reply":"2022-05-17T08:15:13.963259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Function of showing Accuracy and Lost plot</b> </h1>","metadata":{}},{"cell_type":"code","source":"import math\nprint(math.ceil(1.312 * 100) / 100)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:13.965278Z","iopub.execute_input":"2022-05-17T08:15:13.965716Z","iopub.status.idle":"2022-05-17T08:15:13.976941Z","shell.execute_reply.started":"2022-05-17T08:15:13.965675Z","shell.execute_reply":"2022-05-17T08:15:13.975903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def showPlot(title, trainAccuracy, testAccuracy, trainLoss, testLoss, tpr = None, fpr = None):\n    \n    minimumValue = min(min(trainAccuracy), min(testAccuracy))\n    maximumValue = max(max(trainAccuracy), max(testAccuracy))\n    numberOfDivisions = (maximumValue - minimumValue) / 3\n    listOf_Yticks = np.arange(minimumValue, math.ceil(maximumValue * 100) / 100, numberOfDivisions)\n    \n    plt.plot(trainAccuracy,'-o')\n    plt.plot(testAccuracy,'-o')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend(['Train','Test'])\n#     plt.title('Accuracy of ' + title + ' network')\n    plt.yticks(listOf_Yticks)\n    plt.savefig(title + ' accuracy.png')\n    plt.show()\n    \n    minimumValue = min(min(trainLoss), min(testLoss))\n    maximumValue = max(max(trainLoss), max(testLoss))\n    numberOfDivisions = (maximumValue - minimumValue) / 3\n    listOf_Yticks = np.arange(minimumValue, math.ceil(maximumValue * 100) / 100, numberOfDivisions)\n\n    plt.plot(trainLoss,'-o')\n    plt.plot(testLoss,'-o')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(['Train','Test'])\n#     plt.title('Loss of ' + title + ' network')\n    plt.yticks(listOf_Yticks)\n    plt.savefig(title + ' loss.png')\n    plt.show()\n\n    if title != 'Segmentation':\n        area = auc(fpr, tpr)\n        plt.figure()\n        plt.plot(fpr, tpr, label = \"ROC curve (area = %0.5f)\" % area)\n        plt.plot([0,1], [0,1], 'r--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('FPR')\n        plt.ylabel('TPR')\n        plt.legend(loc=4)\n        plt.title('ROC of ' + title + ' network')\n        plt.savefig(title + ' ROC.eps')\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:13.979977Z","iopub.execute_input":"2022-05-17T08:15:13.980245Z","iopub.status.idle":"2022-05-17T08:15:13.993237Z","shell.execute_reply.started":"2022-05-17T08:15:13.980212Z","shell.execute_reply":"2022-05-17T08:15:13.992323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Function of showing Confusion Matrix</b> </h1>","metadata":{}},{"cell_type":"code","source":"def showConfusionMatrix(title, labels, predicts):\n    confusion = confusion_matrix(labels, predicts, labels = [0, 1])\n    report = classification_report(labels, predicts, target_names = [\"Men\", \"Women\"])\n    confusion = np.round(confusion / confusion.astype(np.float).sum(axis=0), 2)\n\n    fig = plt.figure()\n    sn.heatmap(confusion, annot=True)\n    plt.ylabel(\"Actual\")\n    plt.xlabel(\"Predicted\")\n    fig.suptitle(\"Confusion Matrix of \" + title + \" network\", fontsize = 12)\n    plt.show()\n    \n    print(\"\\n\\n\\n\")\n    \n    print(report)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:13.995452Z","iopub.execute_input":"2022-05-17T08:15:13.995897Z","iopub.status.idle":"2022-05-17T08:15:14.00597Z","shell.execute_reply.started":"2022-05-17T08:15:13.995833Z","shell.execute_reply":"2022-05-17T08:15:14.005096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b>Dataset for low and high-frequency images </b> </h1>","metadata":{}},{"cell_type":"code","source":"class LowHighDataset(Dataset):\n    def __init__(self, lowTransform = None, highTransform = None):\n        self.df = read_csv(LABEL_PATH)\n        self.outs_folder = OUT_PATH\n        self.lowTransform = lowTransform\n        self.highTransform = highTransform\n        self.classToIndex = {\"male\":0, \"female\":1}\n\n    def __len__(self):\n        return len(self.df)\n        \n    def __getitem__(self, index):\n        filename = self.df[\"imageName\"][index]\n        label = self.classToIndex[self.df[\"gender\"][index]]\n        \n        lowImg = getPartOfMat(loadmat(self.outs_folder + \"/\" + filename[:-3] + \"mat\"), \"low\")\n        highImg = getPartOfMat(loadmat(self.outs_folder + \"/\" + filename[:-3] + \"mat\"), \"high\")\n        \n        if self.lowTransform is not None:\n            lowImg = self.lowTransform(lowImg)\n            \n        if self.highTransform is not None:\n            highImg = self.highTransform(highImg)\n            \n        return lowImg, highImg, label","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.00854Z","iopub.execute_input":"2022-05-17T08:15:14.009261Z","iopub.status.idle":"2022-05-17T08:15:14.019758Z","shell.execute_reply.started":"2022-05-17T08:15:14.009148Z","shell.execute_reply":"2022-05-17T08:15:14.018994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Transform low-frequency and high-frequency and main images </b> </h1>","metadata":{}},{"cell_type":"code","source":"mainTransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((112, 112)),\n    transforms.Normalize((meanImg1, meanImg2, meanImg3), (stdImg1, stdImg2, stdImg3))\n])\n\nlowTransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((meanLow1, meanLow2, meanLow3), (stdLow1, stdLow2, stdLow3))\n])\n\nhighTransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((meanHigh), (stdHigh))\n])","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.023116Z","iopub.execute_input":"2022-05-17T08:15:14.0234Z","iopub.status.idle":"2022-05-17T08:15:14.03299Z","shell.execute_reply.started":"2022-05-17T08:15:14.023361Z","shell.execute_reply":"2022-05-17T08:15:14.032258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Creating train and test loader for low and high-frequency images </b> </h1>","metadata":{}},{"cell_type":"code","source":"dataset = LowHighDataset(lowTransform, highTransform)\n\nbatch_size = 130\nvalidation_split = 0.7\nshuffle_dataset = True\ndataset_size = len(images)\nsplit = int(np.floor(validation_split * dataset_size))\nindices = list(range(dataset_size))\n\nif shuffle_dataset: np.random.shuffle(indices)\n\ntrain_indices, test_indices = indices[:split], indices[split:]\ntrain_sampler, test_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(test_indices)\n\ntrainLoader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\ntestLoader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.036422Z","iopub.execute_input":"2022-05-17T08:15:14.03682Z","iopub.status.idle":"2022-05-17T08:15:14.06558Z","shell.execute_reply.started":"2022-05-17T08:15:14.036789Z","shell.execute_reply":"2022-05-17T08:15:14.064894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b>Dataset for Segmentation </b> </h1>","metadata":{}},{"cell_type":"code","source":"class EYTHDataset(Dataset):\n    def __init__(self, image_seg_transform = None, mask_transform = None):\n        super(EYTHDataset, self).__init__()\n        self.image_path = EYTH_IMG_PATH\n        self.mask_path = EYTH_MASK_PATH\n        self.image_seg_transform = image_seg_transform\n        self.mask_transform = mask_transform\n        self.segImages = sorted(os.listdir(self.image_path))\n        self.maskImages = sorted(os.listdir(self.mask_path))\n\n    def __len__(self):\n        return len(os.listdir(self.image_path))\n\n    def __getitem__(self, idx):\n\n        segImage = Image.open(self.image_path + self.segImages[idx])\n        maskImage = Image.open(self.mask_path + self.maskImages[idx])\n        if self.image_seg_transform is not None:\n            segImage = self.image_seg_transform(segImage)\n\n        if self.mask_transform is not None:\n            maskImage = self.mask_transform(maskImage)\n            \n        return segImage, maskImage","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.06758Z","iopub.execute_input":"2022-05-17T08:15:14.068225Z","iopub.status.idle":"2022-05-17T08:15:14.076642Z","shell.execute_reply.started":"2022-05-17T08:15:14.068187Z","shell.execute_reply":"2022-05-17T08:15:14.075685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Transform images and masks for Segmentation </b> </h1>","metadata":{}},{"cell_type":"code","source":"image_seg_transform = transforms.Compose([\n    transforms.Resize((112, 112)),\n    transforms.ToTensor(),\n    transforms.Normalize((meanSeg1, meanSeg2, meanSeg3), (stdSeg1, stdSeg2, stdSeg3))\n])\nmask_transform = transforms.Compose([\n    transforms.Resize((112, 112)),\n    transforms.ToTensor(),\n    lambda m: torch.where(m > 0, torch.ones_like(m), torch.zeros_like(m))\n])","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.077894Z","iopub.execute_input":"2022-05-17T08:15:14.078205Z","iopub.status.idle":"2022-05-17T08:15:14.086793Z","shell.execute_reply.started":"2022-05-17T08:15:14.078168Z","shell.execute_reply":"2022-05-17T08:15:14.085935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Creating train and test loader for images and masks for Segmentation </b> </h1>","metadata":{}},{"cell_type":"code","source":"datasetSeg = EYTHDataset(image_seg_transform, mask_transform)\n\nbatch_size = 24\nvalidation_split = 0.8\nshuffle_dataset = True\ndataset_size = len(datasetSeg)\nsplit = int(np.floor(validation_split * dataset_size))\nindices = list(range(dataset_size))\n\nif shuffle_dataset: np.random.shuffle(indices)\n\ntrain_indices, test_indices = indices[:split], indices[split:]\ntrain_sampler, test_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(test_indices)\n\ntrainLoaderSeg = DataLoader(datasetSeg, batch_size=batch_size, sampler=train_sampler)\ntestLoaderSeg = DataLoader(datasetSeg, batch_size=batch_size, sampler=test_sampler)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.087723Z","iopub.execute_input":"2022-05-17T08:15:14.087942Z","iopub.status.idle":"2022-05-17T08:15:14.10171Z","shell.execute_reply.started":"2022-05-17T08:15:14.087909Z","shell.execute_reply":"2022-05-17T08:15:14.100908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b>Dataset for Three-stream </b> </h1>","metadata":{}},{"cell_type":"code","source":"class trippleStreamDataset(Dataset):\n    def __init__(self, mainTransform = None, lowTransform = None, highTransform = None):\n        self.df = read_csv(LABEL_PATH)\n        self.outs_folder = OUT_PATH\n        self.mainTransform = mainTransform\n        self.lowTransform = lowTransform\n        self.highTransform = highTransform\n        self.classToIndex = {\"male\":0, \"female\":1}\n        self.imageLst = os.listdir(IMG_PATH)\n\n    def __len__(self):\n        return len(self.df)\n        \n    def __getitem__(self, index):\n        filename = self.df[\"imageName\"][index]\n        label = self.classToIndex[self.df[\"gender\"][index]]\n        side = self.df[\"aspectOfHand\"][index].split()[0]\n        \n        mainImage = Image.open(IMG_PATH + self.imageLst[index])\n        lowImage = getPartOfMat(loadmat(self.outs_folder + \"/\" + filename[:-3] + \"mat\"), \"low\")\n        highImage = getPartOfMat(loadmat(self.outs_folder + \"/\" + filename[:-3] + \"mat\"), \"high\")\n        \n        if self.mainTransform is not None:\n            mainImage = self.mainTransform(mainImage)\n           \n        if self.lowTransform is not None:\n            lowImage = self.lowTransform(lowImage)\n            \n        if self.highTransform is not None:\n            highImage = self.highTransform(highImage)\n            \n        return mainImage, lowImage, highImage, label, side","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.103383Z","iopub.execute_input":"2022-05-17T08:15:14.103719Z","iopub.status.idle":"2022-05-17T08:15:14.115149Z","shell.execute_reply.started":"2022-05-17T08:15:14.103681Z","shell.execute_reply":"2022-05-17T08:15:14.114063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Creating train and test loader for Three-stream </b> </h1>","metadata":{}},{"cell_type":"code","source":"streamDataset = trippleStreamDataset(mainTransform, lowTransform, highTransform)\n\nbatch_size = 60\nvalidation_split = 0.7\nshuffle_dataset = True\ndataset_size = len(streamDataset)\nsplit = int(np.floor(validation_split * dataset_size))\nindices = list(range(dataset_size))\n\nif shuffle_dataset: np.random.shuffle(indices)\n\ntrain_indices, test_indices = indices[:split], indices[split:]\ntrain_sampler, test_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(test_indices)\n\ntrainLoaderStream = DataLoader(streamDataset, batch_size=batch_size, sampler=train_sampler)\ntestLoaderStream = DataLoader(streamDataset, batch_size=batch_size, sampler=test_sampler)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.116488Z","iopub.execute_input":"2022-05-17T08:15:14.116783Z","iopub.status.idle":"2022-05-17T08:15:14.149349Z","shell.execute_reply.started":"2022-05-17T08:15:14.116744Z","shell.execute_reply":"2022-05-17T08:15:14.148677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Define UNet </b> </h1>","metadata":{}},{"cell_type":"code","source":"class DoubleConv(Module):\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(Module):\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.150514Z","iopub.execute_input":"2022-05-17T08:15:14.150977Z","iopub.status.idle":"2022-05-17T08:15:14.166359Z","shell.execute_reply.started":"2022-05-17T08:15:14.150942Z","shell.execute_reply":"2022-05-17T08:15:14.165589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UNet(Module):\n    def __init__(self, channels, classes, bilinear=True):\n        super(UNet, self).__init__()\n        self.n_channels = channels\n        self.n_classes = classes\n        self.bilinear = bilinear\n\n        self.inc = DoubleConv(channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(512, 1024 // factor)\n        self.up1 = Up(1024, 512 // factor, bilinear)\n        self.up2 = Up(512, 256 // factor, bilinear)\n        self.up3 = Up(256, 128 // factor, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.167711Z","iopub.execute_input":"2022-05-17T08:15:14.168178Z","iopub.status.idle":"2022-05-17T08:15:14.180343Z","shell.execute_reply.started":"2022-05-17T08:15:14.168141Z","shell.execute_reply":"2022-05-17T08:15:14.179628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Define each block of our version of ResNet network </b> </h1>","metadata":{}},{"cell_type":"code","source":"class BasicBlock(Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super().__init__()\n        self.conv1 = Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = BatchNorm2d(planes)\n        self.relu = ReLU(inplace=True)\n        self.conv2 = Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.185761Z","iopub.execute_input":"2022-05-17T08:15:14.186189Z","iopub.status.idle":"2022-05-17T08:15:14.195478Z","shell.execute_reply.started":"2022-05-17T08:15:14.18615Z","shell.execute_reply":"2022-05-17T08:15:14.194643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Define our version of ResNet network </b> </h1>","metadata":{}},{"cell_type":"code","source":"class ResNet(Module):\n    def __init__(self, block, layers, status = \"low\", num_classes=2):\n        super().__init__()\n        \n        self.attention = None\n        \n        self.inplanes = 64\n        \n        inputLayer = 3 if status == \"low\" else 1 if status == \"high\" else None\n\n        self.conv1 = Conv2d(inputLayer, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = BatchNorm2d(self.inplanes)\n        self.relu = ReLU(inplace=True)\n        self.maxpool = MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1])\n        self.layer3 = self._make_layer(block, 256, layers[2])\n        self.layer4 = self._make_layer(block, 512, layers[3])\n        \n        self.avgpool = AdaptiveAvgPool2d((1, 1))\n        self.fc = Sequential(\n            Linear(\n                in_features=512,\n                out_features=num_classes\n            ),\n            Sigmoid()\n        )\n\n    def _make_layer(self, block, planes, blocks, stride=2):\n        downsample = None  \n   \n        if stride != 1 or self.inplanes != planes:\n            downsample = Sequential(\n                Conv2d(self.inplanes, planes, 1, stride, bias=False),\n                BatchNorm2d(planes),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        \n        self.inplanes = planes\n        \n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return Sequential(*layers)\n    \n    def setAttention(self, attention):\n        self.attention = attention\n    \n    def forward(self, x):       \n        x = self.conv1(x)           # 224x224\n        \n        if self.attention != None:\n            x = torch.mul(x, self.attention)\n        \n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)         # 112x112\n\n        x = self.layer1(x)          # 56x56\n        x = self.layer2(x)          # 28x28\n        x = self.layer3(x)          # 14x14\n        x = self.layer4(x)          # 7x7\n        \n        x = self.avgpool(x)         # 1x1\n\n        x = torch.flatten(x, 1)     # remove 1 X 1 grid and make vector of tensor shape \n        x = self.fc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.197722Z","iopub.execute_input":"2022-05-17T08:15:14.198385Z","iopub.status.idle":"2022-05-17T08:15:14.214489Z","shell.execute_reply.started":"2022-05-17T08:15:14.198349Z","shell.execute_reply":"2022-05-17T08:15:14.213783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resnet34(status):\n    layers=[2, 3, 4, 2]\n    model = ResNet(BasicBlock, layers, status)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.217387Z","iopub.execute_input":"2022-05-17T08:15:14.217837Z","iopub.status.idle":"2022-05-17T08:15:14.225659Z","shell.execute_reply.started":"2022-05-17T08:15:14.217799Z","shell.execute_reply":"2022-05-17T08:15:14.224803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Define Three-stream </b> </h1>","metadata":{}},{"cell_type":"code","source":"class trippleStream(Module):\n    def __init__(self, lowNet, highNet, unet):\n        super(trippleStream, self).__init__()\n        self.unet = unet\n        self.lowNet = lowNet\n        self.highNet = highNet\n        self.fc = Linear(4, 2, bias=True)\n    \n    def forward(self, xseg, xlow, xhigh):\n        xseg = self.unet(xseg)\n        \n        xseg = Softmax(dim=1)(xseg)\n        xseg = xseg.unsqueeze(1)\n        xseg = xseg[:,:,1,:,:]\n        xseg = xseg.repeat(1,64,1,1)\n        self.lowNet.setAttention(xseg)\n        self.highNet.setAttention(xseg)\n        \n        xlow = self.lowNet(xlow)\n        xhigh = self.highNet(xhigh)\n        x = torch.cat((xlow, xhigh), dim=1)\n        x = self.fc(F.relu(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.226859Z","iopub.execute_input":"2022-05-17T08:15:14.227212Z","iopub.status.idle":"2022-05-17T08:15:14.237411Z","shell.execute_reply.started":"2022-05-17T08:15:14.227175Z","shell.execute_reply":"2022-05-17T08:15:14.236644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Functions of train and test of segmentation network</b> </h1>","metadata":{}},{"cell_type":"code","source":"def trainSegmentation(epoch, epochs, model, trainLoader, trainAccuracy, trainLoss, opt, criterion):\n    running_loss = 0\n    total = 0\n    correct = 0\n    intersection, union = 0, 0\n    \n    for images, masks in tqdm(trainLoader, desc=\"Train\", colour='green'):\n        \n        if torch.cuda.is_available(): \n            images = images.to(device, dtype=torch.float32)\n            masks = masks.to(device, dtype=torch.long)\n        else:\n            images = images.to(dtype=torch.float32)\n            masks = masks.to(dtype=torch.long)\n            \n        masks = masks.squeeze(1)\n        \n        opt.zero_grad()\n\n        with torch.cuda.amp.autocast(enabled=False):\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            \n            loss.backward()\n            opt.step()\n            \n            _, predicted = torch.max(outputs.data, 1)\n            total += masks.nelement()\n            correct += predicted.eq(masks.data).sum().item()\n            \n            running_loss += loss.item() / len(trainLoader)\n            \n    accuracy = 100 * correct / total\n    trainAccuracy.append(accuracy)\n    trainLoss.append(running_loss)\n    \n    print(\"\\n\" + GREEN + \"TRAIN: Epoch {}/{}, Accuracy: {:.3f}, Loss: {:.3f}\\n\".format(epoch + 1, epochs, accuracy, running_loss))","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.238754Z","iopub.execute_input":"2022-05-17T08:15:14.239209Z","iopub.status.idle":"2022-05-17T08:15:14.251203Z","shell.execute_reply.started":"2022-05-17T08:15:14.239147Z","shell.execute_reply":"2022-05-17T08:15:14.250424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def testSegmentation(epoch, epochs, model, testLoader, testAccuracy, testLoss, opt, criterion):\n    running_loss = 0\n    total = 0\n    correct = 0\n    with torch.no_grad():\n        for images, masks in tqdm(testLoader, desc=\"Train\", colour='red'):\n\n            if torch.cuda.is_available(): \n                images = images.to(device, dtype=torch.float32)\n                masks = masks.to(device, dtype=torch.long)\n            else:\n                images = images.to(dtype=torch.float32)\n                masks = masks.to(dtype=torch.long)\n            \n            masks = masks.squeeze(1)\n\n            with torch.cuda.amp.autocast(enabled=False):\n                outputs = model(images)\n                loss = criterion(outputs, masks)\n\n                _, predicted = torch.max(outputs.data, 1)\n                total += masks.nelement()\n                correct += predicted.eq(masks.data).sum().item()\n\n                running_loss += loss.item() / len(testLoader)\n        \n        accuracy = 100 * correct / total\n        testAccuracy.append(accuracy)\n        testLoss.append(running_loss)\n        \n        print(\"\\n\" + RED + \"TEST: Epoch {}/{}, Accuracy: {:.3f}, Loss: {:.3f}\\n\".format(epoch + 1, epochs, accuracy, running_loss))","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.253833Z","iopub.execute_input":"2022-05-17T08:15:14.254692Z","iopub.status.idle":"2022-05-17T08:15:14.269419Z","shell.execute_reply.started":"2022-05-17T08:15:14.254659Z","shell.execute_reply":"2022-05-17T08:15:14.268765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Functions of train and test of low and high-frequency network</b> </h1>","metadata":{}},{"cell_type":"code","source":"def trainLowHigh(epoch, epochs, model, loader, accuracies, losses, opt, criterion, status):\n    running_loss = 0\n    total, correct = 0, 0\n    for lowInputs, highInputs, labels in tqdm(loader, desc=\"Train\", colour='green'):\n        \n        inputs = lowInputs if status == \"low\" else highInputs if status == \"high\" else None\n                \n        if torch.cuda.is_available(): \n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n        opt.zero_grad()\n        \n        outputs = model(inputs)\n\n        loss = criterion(outputs, labels)\n        loss.backward()\n        opt.step()\n\n        running_loss += loss.item() / len(loader)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\n    accuracy = 100 * correct / total\n    accuracies.append(accuracy)\n    losses.append(running_loss)\n        \n    print(\"\\n\" + GREEN + \"TRAIN: Epoch {}/{}, Accuracy: {:.3f}, Loss: {:.3f}\\n\".format(epoch + 1, epochs, accuracy, running_loss))","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.270708Z","iopub.execute_input":"2022-05-17T08:15:14.271091Z","iopub.status.idle":"2022-05-17T08:15:14.2825Z","shell.execute_reply.started":"2022-05-17T08:15:14.271051Z","shell.execute_reply":"2022-05-17T08:15:14.28176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def testLowHigh(epoch, epochs, model, loader, accuracies, losses, criterion, status):    \n    running_loss = 0\n    total, correct = 0, 0\n    womenPredicts = []\n    allPredicts, allLabels = [] ,[]\n    \n    with torch.no_grad():\n        for lowInputs, highInputs, labels in tqdm(loader, desc=\"Test\", colour='red'):\n\n            inputs = lowInputs if status == \"low\" else highInputs if status == \"high\" else None\n            if torch.cuda.is_available(): \n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n            outputs = model(inputs)\n            \n            loss = criterion(outputs, labels)\n            running_loss += loss.item() / len(loader)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n                        \n            womenPredicts.extend([row[1] for row in outputs.detach().cpu().numpy()])\n            allLabels.extend(labels.detach().cpu().numpy())\n            allPredicts.extend(predicted.detach().cpu().numpy())\n        \n    fpr, tpr, threshold = roc_curve(allLabels, womenPredicts)             \n\n    accuracy = 100 * correct / total\n    accuracies.append(accuracy)\n    losses.append(running_loss)\n    \n    print(\"\\n\" + RED + \"TEST: Epoch {}/{}, Accuracy: {:.3f}, Loss: {:.3f}\\n\".format(epoch + 1, epochs, accuracy, running_loss))\n    \n    return allLabels, allPredicts, tpr, fpr","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.284338Z","iopub.execute_input":"2022-05-17T08:15:14.284849Z","iopub.status.idle":"2022-05-17T08:15:14.298412Z","shell.execute_reply.started":"2022-05-17T08:15:14.284813Z","shell.execute_reply":"2022-05-17T08:15:14.297563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Functions of train and test of three-stream network</b> </h1>","metadata":{}},{"cell_type":"code","source":"def trainStream(epoch, epochs, model, loader, accuracies, losses, opt, criterion):\n    running_loss = 0\n    total, correct = 0, 0\n    \n    for imgInputs, lowInputs, highInputs, labels, sides in tqdm(loader, desc=\"Train\", colour='green'):\n        \n        if torch.cuda.is_available(): \n            imgInputs = imgInputs.to(device)\n            lowInputs = lowInputs.to(device)\n            highInputs = highInputs.to(device)\n            labels = labels.to(device)\n                        \n        opt.zero_grad()\n\n        outputs = model(imgInputs, lowInputs, highInputs)\n\n        loss = criterion(outputs, labels)\n        loss.backward()\n        opt.step()\n\n        running_loss += loss.item() / len(loader)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()  \n\n\n    accuracy = 100 * correct / total\n    \n    accuracies.append(accuracy)\n    losses.append(running_loss)\n            \n    print(\"\\n\" + GREEN + \"TRAIN: Epoch {}/{}, Accuracy: {:.3f}, Loss: {:.3f}\\n\".format(epoch + 1, epochs, accuracy, running_loss))","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.299851Z","iopub.execute_input":"2022-05-17T08:15:14.300373Z","iopub.status.idle":"2022-05-17T08:15:14.311378Z","shell.execute_reply.started":"2022-05-17T08:15:14.300332Z","shell.execute_reply":"2022-05-17T08:15:14.310358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def testStream(epoch, epochs, model, loader, accuracies, losses, criterion):    \n    running_loss = 0\n    total, correct = 0, 0\n    totalPalmar, totalDorsal = 0, 0\n    correctPalmar, correctDorsal = 0, 0\n    womenPredicts = []\n    allPredicts, allLabels = [] ,[]\n\n    with torch.no_grad():\n        for imgInputs, lowInputs, highInputs, labels, sides in tqdm(loader, desc=\"Test\", colour='red'):\n            \n            if torch.cuda.is_available(): \n                imgInputs = imgInputs.to(device)\n                lowInputs = lowInputs.to(device)\n                highInputs = highInputs.to(device)\n                labels = labels.to(device)\n\n            outputs = model(imgInputs, lowInputs, highInputs)\n            \n            loss = criterion(outputs, labels)\n            running_loss += loss.item() / len(loader)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            for person, side in enumerate(sides):\n                if sides[person] == \"palmar\":\n                    totalPalmar += 1\n                    if labels[person] == predicted[person]:\n                        correctPalmar += 1\n                elif sides[person] == \"dorsal\":\n                    totalDorsal += 1\n                    if labels[person] == predicted[person]:\n                        correctDorsal += 1\n\n            womenPredicts.extend([row[1] for row in outputs.detach().cpu().numpy()])\n            allLabels.extend(labels.detach().cpu().numpy())\n            allPredicts.extend(predicted.detach().cpu().numpy())\n        \n    fpr, tpr, threshold = roc_curve(allLabels, womenPredicts)\n\n    accuracy = 100 * correct / total\n    palmarAccuracy = 100 * correctPalmar / totalPalmar\n    dorsalAccuracy = 100 * correctDorsal / totalDorsal\n    \n    accuracies.append(accuracy)\n    losses.append(running_loss)\n            \n    print(\"\\n\" + RED + \"TEST: Epoch {}/{}, Accuracy: {:.3f}, Loss: {:.3f}, Accuracy of dorsal hands: {:.3f}, Accuracy of palmar hands: {:.3f}\\n\".format(epoch + 1, epochs, accuracy, running_loss, dorsalAccuracy, palmarAccuracy))\n    \n    return allLabels, allPredicts, tpr, fpr","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.312979Z","iopub.execute_input":"2022-05-17T08:15:14.313259Z","iopub.status.idle":"2022-05-17T08:15:14.327753Z","shell.execute_reply.started":"2022-05-17T08:15:14.313222Z","shell.execute_reply":"2022-05-17T08:15:14.326994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b> Train and Test hand image segmentations with UNet network</b> </h1>","metadata":{}},{"cell_type":"code","source":"unet = UNet(channels = 3, classes = 2)\nif torch.cuda.is_available():\n    unet = unet.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:14.329238Z","iopub.execute_input":"2022-05-17T08:15:14.329985Z","iopub.status.idle":"2022-05-17T08:15:17.403705Z","shell.execute_reply.started":"2022-05-17T08:15:14.329949Z","shell.execute_reply":"2022-05-17T08:15:17.402912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 35\ncriterion = CrossEntropyLoss()\nopt = RMSprop(unet.parameters(), lr=0.01, momentum=0.9, weight_decay = 1e-7)\nscheduler = StepLR(opt, step_size=12, gamma=0.6)\n\ntrainAccuracy, trainLoss, testAccuracy, testLoss = [], [], [], []\n\nfor epoch in range(epochs):\n    trainSegmentation(epoch, epochs, unet, trainLoaderSeg, trainAccuracy, trainLoss, opt, criterion)\n    testSegmentation(epoch, epochs, unet, testLoaderSeg, testAccuracy, testLoss, opt, criterion)\n    scheduler.step()\n    deleteRedundantCache(unet)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:15:17.405185Z","iopub.execute_input":"2022-05-17T08:15:17.405459Z","iopub.status.idle":"2022-05-17T08:17:10.646388Z","shell.execute_reply.started":"2022-05-17T08:15:17.405421Z","shell.execute_reply":"2022-05-17T08:17:10.64503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"showPlot('Segmentation', trainAccuracy, testAccuracy, trainLoss, testLoss)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:17:10.647424Z","iopub.status.idle":"2022-05-17T08:17:10.648492Z","shell.execute_reply.started":"2022-05-17T08:17:10.648239Z","shell.execute_reply":"2022-05-17T08:17:10.648266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b>Train and Test Resnet for low-frequency images </b> </h1>","metadata":{}},{"cell_type":"code","source":"resnetLow = resnet34(\"low\")\nif torch.cuda.is_available(): resnetLow = resnetLow.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:17:13.836228Z","iopub.execute_input":"2022-05-17T08:17:13.836886Z","iopub.status.idle":"2022-05-17T08:17:13.973366Z","shell.execute_reply.started":"2022-05-17T08:17:13.836823Z","shell.execute_reply":"2022-05-17T08:17:13.972624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 15\ncriterion = CrossEntropyLoss()\nopt = SGD(resnetLow.parameters(), lr=0.09, momentum=0.9, weight_decay = 1e-3)\nscheduler = StepLR(opt, step_size=5, gamma=0.7)\n\ntrainAccuracy, trainLoss, testAccuracy, testLoss  = [], [], [], []\n\nfor epoch in range(epochs):\n    trainLowHigh(epoch, epochs, resnetLow, trainLoader, trainAccuracy, trainLoss, opt, criterion, \"low\")\n    labels, predicts, tpr, fpr = testLowHigh(epoch, epochs, resnetLow, testLoader, testAccuracy, testLoss, criterion, \"low\")\n    scheduler.step()\n    deleteRedundantCache(resnetLow)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:17:15.556256Z","iopub.execute_input":"2022-05-17T08:17:15.557018Z","iopub.status.idle":"2022-05-17T08:32:20.199743Z","shell.execute_reply.started":"2022-05-17T08:17:15.556974Z","shell.execute_reply":"2022-05-17T08:32:20.199051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"showConfusionMatrix('low-frequency images', labels, predicts)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:32:20.20154Z","iopub.execute_input":"2022-05-17T08:32:20.202076Z","iopub.status.idle":"2022-05-17T08:32:20.4669Z","shell.execute_reply.started":"2022-05-17T08:32:20.202039Z","shell.execute_reply":"2022-05-17T08:32:20.465627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"showPlot('low-frequency images', trainAccuracy, testAccuracy, trainLoss, testLoss, tpr, fpr)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:32:20.468359Z","iopub.execute_input":"2022-05-17T08:32:20.468644Z","iopub.status.idle":"2022-05-17T08:32:21.320459Z","shell.execute_reply.started":"2022-05-17T08:32:20.468605Z","shell.execute_reply":"2022-05-17T08:32:21.319708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> <b>Train and Test Resnet for high-frequency images </b> </h1>","metadata":{}},{"cell_type":"code","source":"resnetHigh = resnet34(\"high\")\nif torch.cuda.is_available(): resnetHigh = resnetHigh.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:32:21.35772Z","iopub.execute_input":"2022-05-17T08:32:21.357992Z","iopub.status.idle":"2022-05-17T08:32:21.482608Z","shell.execute_reply.started":"2022-05-17T08:32:21.357962Z","shell.execute_reply":"2022-05-17T08:32:21.481899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 15\ncriterion = CrossEntropyLoss()\nopt = SGD(resnetHigh.parameters(), lr=0.08, momentum=0.9, weight_decay = 1e-3)\nscheduler = StepLR(opt, step_size=5, gamma=0.7)\n\ntrainAccuracy, trainLoss, testAccuracy, testLoss = [], [], [], []\n\nfor epoch in range(epochs):\n    trainLowHigh(epoch, epochs, resnetHigh, trainLoader, trainAccuracy, trainLoss, opt, criterion, \"high\")\n    labels, predicts, tpr, fpr = testLowHigh(epoch, epochs, resnetHigh, testLoader, testAccuracy, testLoss, criterion, \"high\")\n    scheduler.step()\n    deleteRedundantCache(resnetHigh)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:32:21.483985Z","iopub.execute_input":"2022-05-17T08:32:21.484272Z","iopub.status.idle":"2022-05-17T08:46:44.216731Z","shell.execute_reply.started":"2022-05-17T08:32:21.484236Z","shell.execute_reply":"2022-05-17T08:46:44.215799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"showConfusionMatrix('high-frequency images', labels, predicts)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:46:44.218022Z","iopub.execute_input":"2022-05-17T08:46:44.21834Z","iopub.status.idle":"2022-05-17T08:46:44.479912Z","shell.execute_reply.started":"2022-05-17T08:46:44.218302Z","shell.execute_reply":"2022-05-17T08:46:44.479105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"showPlot('high-frequency images', trainAccuracy, testAccuracy, trainLoss, testLoss, tpr, fpr)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T08:46:44.481447Z","iopub.execute_input":"2022-05-17T08:46:44.483029Z","iopub.status.idle":"2022-05-17T08:46:45.312391Z","shell.execute_reply.started":"2022-05-17T08:46:44.482982Z","shell.execute_reply":"2022-05-17T08:46:45.310391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <h1> <b> Train and Test for tripple stream</b> </h1>","metadata":{}},{"cell_type":"code","source":"unet = UNet(channels = 3, classes = 2)\n\nthreeStream = trippleStream(resnetLow, resnetHigh, unet)\n\nif torch.cuda.is_available():\n    unet = unet.to(device)\n    threeStream = threeStream.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:59:37.855277Z","iopub.execute_input":"2022-04-15T10:59:37.856835Z","iopub.status.idle":"2022-04-15T10:59:38.09686Z","shell.execute_reply.started":"2022-04-15T10:59:37.856796Z","shell.execute_reply":"2022-04-15T10:59:38.096125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 25\ncriterion = CrossEntropyLoss()\nopt = SGD(threeStream.parameters(), lr=0.01, momentum=0.9, weight_decay = 1e-4)\nscheduler = StepLR(opt, step_size=5, gamma=0.8)\n\ntrainAccuracy, trainLoss, testAccuracy, testLoss = [], [], [], []\n\nfor epoch in range(epochs):\n    trainStream(epoch, epochs, threeStream, trainLoaderStream, trainAccuracy, trainLoss, opt, criterion)\n    labels, predicts, tpr, fpr = testStream(epoch, epochs, threeStream, testLoaderStream, testAccuracy, testLoss, criterion)\n    scheduler.step()\n    deleteRedundantCache(threeStream)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:59:38.098047Z","iopub.execute_input":"2022-04-15T10:59:38.098397Z","iopub.status.idle":"2022-04-15T14:31:52.680627Z","shell.execute_reply.started":"2022-04-15T10:59:38.098355Z","shell.execute_reply":"2022-04-15T14:31:52.679909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"showConfusionMatrix('three-stream', labels, predicts)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T14:31:52.682162Z","iopub.execute_input":"2022-04-15T14:31:52.682641Z","iopub.status.idle":"2022-04-15T14:31:53.016908Z","shell.execute_reply.started":"2022-04-15T14:31:52.682602Z","shell.execute_reply":"2022-04-15T14:31:53.016116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"showPlot('three-stream', trainAccuracy, testAccuracy, trainLoss, testLoss, tpr, fpr)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T14:31:53.020887Z","iopub.execute_input":"2022-04-15T14:31:53.021187Z","iopub.status.idle":"2022-04-15T14:31:53.864559Z","shell.execute_reply.started":"2022-04-15T14:31:53.021147Z","shell.execute_reply":"2022-04-15T14:31:53.863914Z"},"trusted":true},"execution_count":null,"outputs":[]}]}